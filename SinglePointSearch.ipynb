{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from pertubativeHeuristics.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as db\n",
    "import pandas as pd\n",
    "from sqlalchemy import Column, Integer, Text, ForeignKey,String,Table, DateTime\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import datetime\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from operator import attrgetter\n",
    "import math\n",
    "import statistics\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from platform import python_version\n",
    "import import_ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pertubativeHeuristics\n",
    "from pertubativeHeuristics import pertubativeHeuristic, createSolution,genInitialSolution, EvaluateSolution,populateDB,getCurrentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"test.exam\"\n",
    "sample_one_early =\"./itc2007_dataset/exam_comp_set4.exam\" #done\n",
    "sample_two_early =\"./itc2007_dataset/exam_comp_set1.exam\" #done\n",
    "\n",
    "sample_one_late = \"./itc2007_dataset/exam_comp_set6.exam\"#done\n",
    "sample_two_late = \"./itc2007_dataset/exam_comp_set8.exam\"#done\n",
    "\n",
    "\n",
    "sample_one_hidden = \"./itc2007_dataset/exam_comp_set9.exam\"#done\n",
    "sample_two_hidden = \"./itc2007_dataset/exam_comp_set12.exam\"#done\n",
    "\n",
    "sample = sample_one_late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine = db.create_engine('postgresql://postgres:password@postgres:5432/postgres')\n",
    "engine = db.create_engine('postgresql://postgres:password@postgres:5432/postgres')\n",
    "connection = engine.connect()\n",
    "meta = db.MetaData(connection)\n",
    "Base = declarative_base()\n",
    "Session = sessionmaker(bind = engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period_room\n",
      "exam_student\n",
      "exam_period\n",
      "student\n",
      "room\n",
      "period\n",
      "exam\n",
      "period ['1', ' EXAM_COINCIDENCE', ' 8\\n']\n",
      "period ['2', ' EXAM_COINCIDENCE', ' 10\\n']\n",
      "period ['13', ' EXAM_COINCIDENCE', ' 40\\n']\n",
      "period ['34', ' EXAM_COINCIDENCE', ' 78\\n']\n",
      "period ['35', ' EXAM_COINCIDENCE', ' 80\\n']\n",
      "period ['82', ' EXAM_COINCIDENCE', ' 99\\n']\n",
      "period ['100', ' EXAM_COINCIDENCE', ' 120\\n']\n",
      "period ['111', ' EXAM_COINCIDENCE', ' 122\\n']\n",
      "period ['121', ' EXAM_COINCIDENCE', ' 134\\n']\n",
      "period ['126', ' EXAM_COINCIDENCE', ' 136\\n']\n",
      "period ['140', ' EXAM_COINCIDENCE', ' 149\\n']\n",
      "period ['150', ' EXAM_COINCIDENCE', ' 151\\n']\n",
      "period ['152', ' EXAM_COINCIDENCE', ' 152\\n']\n",
      "period ['154', ' EXAM_COINCIDENCE', ' 162\\n']\n",
      "period ['163', ' EXAM_COINCIDENCE', ' 165\\n']\n",
      "period ['166', ' EXAM_COINCIDENCE', ' 170\\n']\n",
      "period ['173', ' EXAM_COINCIDENCE', ' 189\\n']\n",
      "period ['179', ' EXAM_COINCIDENCE', ' 186\\n']\n",
      "period ['190', ' EXAM_COINCIDENCE', ' 199\\n']\n",
      "period ['151', ' AFTER', ' 3\\n']\n",
      "period ['33', ' AFTER', ' 5\\n']\n",
      "period ['123', ' EXCLUSION', ' 121\\n']\n",
      "period ['166', ' EXCLUSION', ' 18\\n']\n",
      "['TWOINAROW', '20']\n",
      "['TWOINADAY', '5']\n",
      "['PERIODSPREAD', '20']\n",
      "['NONMIXEDDURATIONS', '25']\n",
      "['FRONTLOAD', '25', '30', '15']\n"
     ]
    }
   ],
   "source": [
    "softconstraints,constraints,examRows,periodRows,period_count = populateDB(engine,session,Base,connection,sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of hard constraint violations 0\n",
      "iteration: 1\n"
     ]
    }
   ],
   "source": [
    "genInitialSolution(connection,session,constraints,examRows,periodRows,sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1425"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentScore = getCurrentScore(softconstraints,connection)\n",
    "currentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(index, utility_scores):\n",
    "    utility_scores[index-1]  = utility_scores[index-1] + 0.5\n",
    "def punish(index, utility_scores):\n",
    "    utility_scores[index-1]  = utility_scores[index-1] - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scores = []\n",
    "for i in tqdm(range(10)):\n",
    "    iteration_scores = []\n",
    "    utility_Scores = [1,1,1,1,1,1,1,1,1]\n",
    "    \n",
    "    currentScore = getCurrentScore(softconstraints,connection)\n",
    "    violationCount = EvaluateSolution(softconstraints,connection)\n",
    "    iteration_count = 0\n",
    "   \n",
    "    quality_lower_bound = currentScore\n",
    "    while iteration_count < 10:\n",
    "        iteration_count = iteration_count + 1\n",
    "        heuristic= utility_Scores.index(max(utility_Scores)) + 1\n",
    "        \n",
    "        indices = [i for i, x in enumerate(utility_Scores) if x == max(utility_Scores)]\n",
    "#         print(\"indices:\",indices)\n",
    "        if len(indices) > 1:\n",
    "            heuristic = random.choice(indices)\n",
    "   \n",
    "        pertubativeHeuristic(heuristic,period_count,connection)\n",
    "        score = getCurrentScore(softconstraints,connection)\n",
    "        iteration_scores.append(score)\n",
    "        violationCount = EvaluateSolution(constraints,connection)\n",
    "\n",
    "\n",
    "        if violationCount > 0:\n",
    "            rollback_query = db.text(\"rollback to pre_heuristic;\")\n",
    "            connection.execute(rollback_query)\n",
    "            violationCount = EvaluateSolution(constraints,connection)        \n",
    "        elif score < currentScore and violationCount == 0:\n",
    "            reward(heuristic,utility_Scores)\n",
    "            currentScore = score\n",
    "            iteration_count = 0\n",
    "\n",
    "\n",
    "        elif violationCount == 0 and score > currentScore:\n",
    "            punish(heuristic,utility_Scores)\n",
    "            # the threshold decreases\n",
    "            if score < quality_lower_bound + (currentScore - quality_lower_bound)*(1 - i/50):\n",
    "                    currentScore = score\n",
    "                    iteration_count = 0\n",
    "            else:\n",
    "                rollback_query = db.text(\"rollback to pre_heuristic;\")\n",
    "                connection.execute(rollback_query)\n",
    "                \n",
    "    run_scores.append(iteration_scores)\n",
    "    commit_query = db.text(\"rollback work;\")\n",
    "    connection.execute(commit_query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in run_scores:\n",
    "    score.insert(0,285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(run_scores[1],label='run 1')\n",
    "plt.plot(run_scores[2],label='run 2')\n",
    "plt.plot(run_scores[3],label='run 3')\n",
    "plt.plot(run_scores[4], label='run 4')\n",
    "plt.plot(run_scores[5], label='run 5')\n",
    "plt.plot(run_scores[5], label='run 6')\n",
    "\n",
    "\n",
    "plt.ylabel('objective score')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend();\n",
    "\n",
    "plt.title(\"exam set 6 single point search runs\")\n",
    "plt.show()\n",
    "#fig.savefig('./stopping_condition_20.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score in run_scores:\n",
    "    temp = np.asarray(score)\n",
    "    new_scores.append(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = run_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_arr = np.asarray(scores)\n",
    "scores_arr =  scores_arr.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"result for :\", sample)\n",
    "# print(\"Objective scores:\",scores)\n",
    "print(\"mean:\",np.mean(scores_arr))\n",
    "print(\"std:\",np.std(new_scores))\n",
    "print (\"min:\",np.min(scores_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}